{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c7d0LKCanFK"
   },
   "source": [
    "\n",
    "\n",
    "># **Creation of csv files use in trainng**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F45Zw1nsSqBQ"
   },
   "source": [
    "Mount Google Drive where have data and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJb5nLdFSjcY",
    "outputId": "f5d7d943-b7e6-4c46-c0b5-a89fce007840"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6ehGMGFTCxc"
   },
   "source": [
    "Creating csv files of rumi dataset (already created and shared with drive no need to run this shell anymore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJVVaPNfpxq1"
   },
   "source": [
    "Replace 7 with ä and Word boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83zn7u1quycP"
   },
   "source": [
    "# Create key = path of file duration, and text lists to create csv later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnAkari6S7-s",
    "outputId": "8dd2a2c7-e36a-4997-a39f-9b57706b382f"
   },
   "outputs": [],
   "source": [
    "import wave\n",
    "import os\n",
    "key=[]\n",
    "duration=[]\n",
    "text=[]\n",
    "file = open('/content/drive/MyDrive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt')\n",
    "data_=file.readlines()\n",
    "for i in data_:\n",
    "    # name = i.split()\n",
    "    text.append(i)\n",
    "    \n",
    "\n",
    "a = os.listdir('/content/drive/MyDrive/UrduPhoneticSpeechCorpus/wav')\n",
    "for i in a:\n",
    "  key.append(\"/content/drive/MyDrive/UrduPhoneticSpeechCorpus/wav/\" + i)\n",
    "\n",
    "  location='/content/drive/MyDrive/UrduPhoneticSpeechCorpus/wav/' + i \n",
    "  # print('processing: ',location)\n",
    "  # for j in os.listdir(location):\n",
    "  print(i)\n",
    "  audio = wave.open(location)\n",
    "  # key.append(location)\n",
    "  # text.append(\" \".join(name[1:]))\n",
    "  frames = audio.getnframes()\n",
    "  rate = audio.getframerate()\n",
    "  dur = frames / float(rate)\n",
    "  # print(duration)\n",
    "  audio.close()\n",
    "  duration.append(dur)\n",
    "#     print(location)\n",
    "  \n",
    "    \n",
    "file.close()\n",
    "    \n",
    "# print(key)\n",
    "# print(duration)\n",
    "# print(text)\n",
    "#/content/drive/MyDrive/UrduPhoneticSpeechCorpus/wav/c1.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZUOd5uiTQV-"
   },
   "source": [
    "Created csv files of CSALT dataset (already created and shared with drive no need to run this shell anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xmpxmv-JvBdP"
   },
   "outputs": [],
   "source": [
    "# with open(r'/content/drive/My Drive/UrduPhoneticSpeechCorpus/Transcription-CISAMPA.txt', 'r') as infile:\n",
    "#   with open(r'/content/drive/My Drive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt', 'w') as outfile:\n",
    "#       data = infile.read()\n",
    "#       data = data.replace(\"<s>\", \"\")\n",
    "#       # data = data.replace(\"##\", \" \")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       data = data.replace(\"</s>\", \"\")\n",
    "#       outfile.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkyApTwM4949",
    "outputId": "e377f322-1c38-4585-fb96-ba67d7446524"
   },
   "outputs": [],
   "source": [
    "file = open(\"/content/drive/My Drive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt\",\"r\") \n",
    "Counter = 0\n",
    "  \n",
    "# Reading from file \n",
    "Content = file.read() \n",
    "CoList = Content.split(\"\\n\") \n",
    "  \n",
    "for i in CoList: \n",
    "    if i: \n",
    "        Counter += 1\n",
    "          \n",
    "print(\"This is the number of lines in the file\") \n",
    "print(Counter) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9WTDyknMbQg",
    "outputId": "fef0cb9d-76bd-4f83-d33f-75916f622069"
   },
   "outputs": [],
   "source": [
    "# read file word by word \n",
    "listish =['P','P_H','B','B_H','M','M_H','T_D'\n",
    ",'T_D_H'\n",
    ",'D_D'\n",
    ",'D_D_H'\n",
    ",'TT'\n",
    ",'TT_H'\n",
    ",'DD'\n",
    ",'DD_H'\n",
    ",'N'\n",
    ",'N_H'\n",
    ",'K'\n",
    ",'K_H'\n",
    ",'G'\n",
    ",'G_H'\n",
    ",'NG'\n",
    ",'NG_H'\n",
    ",'Q'\n",
    ",'Y'\n",
    ",'F'\n",
    ",'V'\n",
    ",'S'\n",
    ",'Z'\n",
    ",'SH'\n",
    ",'ZZ'\n",
    ",'X'\n",
    ",'7'\n",
    ",'H'\n",
    ",'L'\n",
    ",'L_H'\n",
    ",'R'\n",
    ",'R_H'\n",
    ",'RR'\n",
    ",'RR_H'\n",
    ",'J'\n",
    ",'J_H'\n",
    ",'T_SH'\n",
    ",'T_SH_H'\n",
    ",'D_ZZ'\n",
    ",'D_ZZ_H'\n",
    ",'UU'\n",
    ",'UUN'\n",
    ",'OO'\n",
    ",'OON'\n",
    ",'O'\n",
    ",'ON'\n",
    ",'AA'\n",
    ",'AAN'\n",
    ",'II'\n",
    ",'IIN'\n",
    ",'AE'\n",
    ",'AEN'\n",
    ",'E'\n",
    ",'AY'\n",
    ",'AYN'\n",
    ",'I'\n",
    ",'U'\n",
    ",'A'\n",
    ",'##']\n",
    "# opening the text file \n",
    "with open('/content/drive/My Drive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt','r') as file: \n",
    "   \n",
    "    # reading each line     \n",
    "    for line in file: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split(): \n",
    "   \n",
    "            # displaying the words  \n",
    "            if word not in listish:          \n",
    "              print(word)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUBtmEQevCE6"
   },
   "source": [
    "# Append path, duration , text to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqkrAptlS0nW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# file = open('/content/drive/My Drive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt')\n",
    "# data_=file.readlines()\n",
    "# for count,i in enumerate(data_):\n",
    "#     #print(i,count)\n",
    "#     # name = i.split()\n",
    "#     location='/content/drive/My Drive/UrduPhoneticSpeechCorpus/wav/c'+str(count+1)+'.wav'\n",
    "#     # print('processing: ',location)\n",
    "#     # for j in os.listdir(location):\n",
    "#     audio = wave.open(location)\n",
    "#     key.append(location)\n",
    "#     text.append(i)\n",
    "#     frames = audio.getnframes()\n",
    "#     rate = audio.getframerate()\n",
    "#     dur = frames / float(rate)\n",
    "#     # print(duration)\n",
    "#     audio.close()\n",
    "#     duration.append(dur)\n",
    "# #     print(location)\n",
    "# file.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKKx5E5zTsdP"
   },
   "source": [
    "The shell below will generate csv file of rumi and csalt dataset and will split them in train,validate and test (60%,20%,20%) set which i shared through google drive (this may help in creatiion of more dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB1Ltj7PvIeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pBq661PvI14"
   },
   "source": [
    "# Create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuUNsEUjltmI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3UXGca6l054"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr_6O57jvhax"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLmtt9nHvhy5"
   },
   "source": [
    "Creating splits using df.sample using length of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-HDnPRFTdvd"
   },
   "outputs": [],
   "source": [
    "# di = {'key':key,'duration':duration,'text':text}\n",
    "# df = pd.DataFrame(di)\n",
    "# df.to_csv(\"/content/drive/My Drive/phoneme_wba_rumi_csalt.csv\",index=False)\n",
    "# train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "# train.to_csv(\"/content/drive/My Drive/phoneme_wba_train_rumi_csalt.csv\",index=False)\n",
    "# validate.to_csv(\"/content/drive/My Drive/phoneme_wba_validate_rumi_csalt.csv\",index=False)\n",
    "# test.to_csv(\"/content/drive/My Drive/phoneme_wba_test_rumi_csalt.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O28aTrE5UMuX"
   },
   "source": [
    "when model reached at WER of 0.3 model wass suffering to train further on this data then i added test set into training set and generate a new csv file which now have 80% of data in training and 20% in validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P27lep4GT7eJ"
   },
   "outputs": [],
   "source": [
    "test='/content/drive/MyDrive/WordBoundaries/WordBound-rumi_csalt.csv'\n",
    "train='/content/drive/MyDrive/WordBoundaries/WordBound-test_rumi_csalt.csv'\n",
    "test_ = pd.read_csv(test)\n",
    "train_ = pd.read_csv(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCZ3YOb7VBHe"
   },
   "source": [
    "________________________________________________________________________________\n",
    "\n",
    "\n",
    "># **Traing Code starts from here**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dZc_ubaVGbK",
    "outputId": "bff419f6-a41a-4eaa-d35b-ac8c139027fc"
   },
   "outputs": [],
   "source": [
    "!pip install pympler\n",
    "!pip install python_speech_features\n",
    "!pip install aubio\n",
    "!pip install kenlm\n",
    "!pip install soundfile\n",
    "!pip uninstall tensorflow tensorflow-gpu protocol --yes\n",
    "\n",
    "!pip install tensorflow-gpu==1.14.0\n",
    "\n",
    "!pip install keras==2.2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tJfP043VTup"
   },
   "source": [
    "When all module are installed click this link:- \n",
    "/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\n",
    "a file will appear, after line no. 4194 add this line:\n",
    "**ignore_longer_outputs_than_inputs=True,** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rugAoLBPd-0C"
   },
   "source": [
    "Declaration of our Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNpqrax8V2iL"
   },
   "outputs": [],
   "source": [
    "\n",
    "char_map_str = \"\"\"\n",
    "0 <SPACE>\n",
    "1 P\n",
    "2 P_H\n",
    "3 B\n",
    "4 B_H\n",
    "5 M\n",
    "6 M_H\n",
    "7 T_D\n",
    "8 T_D_H\n",
    "9 D_D\n",
    "10 D_D_H\n",
    "11 TT\n",
    "12 TT_H\n",
    "13 DD\n",
    "14 DD_H\n",
    "15 N\n",
    "16 N_H\n",
    "17 K\n",
    "18 K_H\n",
    "19 G\n",
    "20 G_H\n",
    "21 NG\n",
    "22 NG_H\n",
    "23 Q\n",
    "24 Y\n",
    "25 F\n",
    "26 V\n",
    "27 S\n",
    "28 Z\n",
    "29 SH\n",
    "30 ZZ\n",
    "31 X\n",
    "32 7\n",
    "33 H\n",
    "34 L\n",
    "35 L_H\n",
    "36 R\n",
    "37 R_H\n",
    "38 RR\n",
    "39 RR_H\n",
    "40 J\n",
    "41 J_H\n",
    "42 T_SH\n",
    "43 T_SH_H\n",
    "44 D_ZZ\n",
    "45 D_ZZ_H\n",
    "46 UU\n",
    "47 UUN\n",
    "48 OO\n",
    "49 OON\n",
    "50 O\n",
    "51 ON\n",
    "52 AA\n",
    "53 AAN\n",
    "54 II\n",
    "55 IIN\n",
    "56 AE\n",
    "57 AEN\n",
    "58 E\n",
    "59 AY\n",
    "60 AYN\n",
    "61 I\n",
    "62 U\n",
    "63 A\n",
    "64 ä\n",
    "65 ##\n",
    "'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "\n",
    "for line in char_map_str.strip().split('\\n')[0:-1]:\n",
    "    try:\n",
    "        ch, index = line.split()\n",
    "    #     print('ch ',ch,' index ',index)\n",
    "        char_map[ch.strip()] = int(index)\n",
    "        index_map[int(index)] = ch.strip()\n",
    "    except:\n",
    "        # print('ch ',ch,' index ',index)\n",
    "        char_map[index.strip()] = int(ch) \n",
    "        index_map[int(ch)] = index.strip()\n",
    "       \n",
    "#         break\n",
    "\n",
    "index_map[0] = ' '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKi9kmaGd3D5"
   },
   "source": [
    "**This will read csv files of audio dataset and its transcription**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8LXrp-OV8jY"
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import pandas as pd\n",
    "# import char_map\n",
    "# from utils import text_to_int_sequence\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def clean(word):\n",
    "    # token = re.compile(\"[\\w-]+|'m|'t|'ll|'ve|'d|'s|\\'\")\n",
    "    ## LC ALL & strip fullstop, comma and semi-colon which are not required\n",
    "    new = word.lower().replace('.', '')\n",
    "    new = new.replace(',', '')\n",
    "    new = new.replace(';', '')\n",
    "    new = new.replace('\"', '')\n",
    "    new = new.replace('!', '')\n",
    "    new = new.replace('?', '')\n",
    "    new = new.replace(':', '')\n",
    "    new = new.replace('-', '')\n",
    "    return new\n",
    "\n",
    "\n",
    "def combine_all_wavs_and_trans_from_csvs(csvslist, sortagrad=True, createwordlist=False, delBigTranscripts=True,start=-100,end=100):\n",
    "    '''Assume that data is in csv already exists with data in form\n",
    "        path, size, transcript\n",
    "        this is best approach for loading in moz deepspeech processed files.\n",
    "    '''\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "  \n",
    "    for csv in csvslist.split(','):\n",
    "        print(\"Reading csv:\",csv)\n",
    "\n",
    "        if os.path.isfile(csv):\n",
    "            try:\n",
    "                df_new = pd.read_csv(csv, sep=',', encoding='ascii')\n",
    "            except:\n",
    "                print(\"NOT - ASCII, use UTF-8\")\n",
    "                df_new = pd.read_csv(csv, sep=',', encoding='utf-8')\n",
    "                # df_new.text.replace({r'[^\\x00-\\x7F]+': ''}, regex=True, inplace=True)\n",
    "\n",
    "            df_all = df_all.append(df_new)\n",
    "    # df_all=df_all[0:100]\n",
    "    print(\"Finished reading in data\")\n",
    "\n",
    "    if delBigTranscripts:\n",
    "        print(\"removing any sentences that are too big- tweetsize\")\n",
    "        df_final = df_all[df_all['text'].map(len) <= 140]\n",
    "    else:\n",
    "        df_final = df_all\n",
    "\n",
    "    # can output the word list here if required\n",
    "    if createwordlist:\n",
    "        df_final['text'].to_csv(\"./lm/df_all_word_list.csv\", sep=',', header=False, index=False)  # reorder + out\n",
    "\n",
    "    listcomb = df_all['text'].tolist()\n",
    "    print(\"Total number of files:\", len(listcomb))\n",
    "\n",
    "\n",
    "    listcomb = df_final['text'].tolist()\n",
    "    print(\"Total number of files (after reduction):\", len(listcomb))\n",
    "\n",
    "    comb = []\n",
    "\n",
    "    for t in listcomb:\n",
    "        #s = t.decode('utf-8').encode('ascii', errors='ignore')\n",
    "        comb.append(' '.join(t.split()))\n",
    "\n",
    "    # print(\"Train/Test/Valid:\",len(train_list_wavs), len(test_list_wavs), len(valid_list_wavs))\n",
    "    # 6300 TIMIT\n",
    "    # (4620, 840, 840) TIMIT\n",
    "\n",
    "\n",
    "\n",
    "    ## SIZE CHECKS\n",
    "    max_intseq_length = get_max_intseq(comb)\n",
    "    num_classes = get_number_of_char_classes()\n",
    "\n",
    "    print(\"max_intseq_length:\", max_intseq_length)\n",
    "    print(\"numclasses:\", num_classes)\n",
    "\n",
    "    # VOCAB CHECKS\n",
    "    all_words, max_trans_charlength = get_words(comb)\n",
    "    print(\"max_trans_charlength:\", max_trans_charlength)\n",
    "    # ('max_trans_charlength:', 80)\n",
    "\n",
    "    ## TODO could readd the mfcc checks for safety\n",
    "    # ('max_mfcc_len:', 778, 'at comb index:', 541)\n",
    "\n",
    "    all_vocab = set(all_words)\n",
    "    print(\"Words:\", len(all_words))\n",
    "    print(\"Vocab:\", len(all_vocab))\n",
    "\n",
    "    dataproperties = {\n",
    "        'target': \"timit+librispeech\",\n",
    "        'num_classes': num_classes,\n",
    "        'all_words': all_words,\n",
    "        'all_vocab': all_vocab,\n",
    "        'max_trans_charlength': max_trans_charlength,\n",
    "        'max_intseq_length': max_intseq_length\n",
    "    }\n",
    "\n",
    "    if sortagrad:\n",
    "        df_final = df_final.sort_values(by='duration', ascending=True)\n",
    "    else:\n",
    "        df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #remove mem\n",
    "    del df_all\n",
    "    del listcomb\n",
    "\n",
    "    return dataproperties, df_final\n",
    "\n",
    "\n",
    "##DATA CHECKS RUN ALL OF THESE\n",
    "\n",
    "def get_words(comb):\n",
    "    max_trans_charlength = 0\n",
    "    all_words = []\n",
    "\n",
    "    for count, sent in enumerate(comb):\n",
    "        # count length\n",
    "        if len(sent) > max_trans_charlength:\n",
    "            max_trans_charlength = len(sent)\n",
    "        # build vocab\n",
    "        for w in sent.split():\n",
    "            all_words.append(clean(w))\n",
    "\n",
    "    return all_words, max_trans_charlength\n",
    "\n",
    "def get_max_intseq(comb):\n",
    "    max_intseq_length = 0\n",
    "    for x in comb:\n",
    "        try:\n",
    "            y = text_to_int_sequence(x)\n",
    "            if len(y) > max_intseq_length:\n",
    "                max_intseq_length = len(y)\n",
    "        except:\n",
    "            print(\"error at:\", x)\n",
    "    return max_intseq_length\n",
    "\n",
    "def get_number_of_char_classes():\n",
    "    ## TODO would be better to check with dataset (once cleaned)\n",
    "    num_classes = len(char_map)+1 ##need +1 for ctc null char +1 pad\n",
    "    return num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZWIcgEPdm98"
   },
   "source": [
    "**Following functions will convert text to integer list ,integer list to text and load previous saved models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kN6dPumpWBWo",
    "outputId": "d5c37191-77f0-4fe2-fa4f-835d0fb0f618"
   },
   "outputs": [],
   "source": [
    "# from char_map import char_map, index_map\n",
    "\n",
    "\n",
    "from pympler import muppy, summary, tracker, classtracker\n",
    "from pympler.garbagegraph import GarbageGraph, start_debug_garbage\n",
    "from pympler.web import start_profiler, start_in_background\n",
    "import types\n",
    "\n",
    "import resource\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import model_from_json, load_model\n",
    "import keras.backend as K\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import yaml\n",
    "\n",
    "# from model import clipped_relu, selu\n",
    "\n",
    "# these text/int characters are modified\n",
    "# from the DS2 github.com/baidu-research/ba-dls-deepspeech\n",
    "\n",
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "    int_sequence = []\n",
    "    \n",
    "    for c in text:\n",
    "\n",
    "      append=True    \n",
    "      if c == ' ':\n",
    "          ch = char_map['<SPACE>']\n",
    "      else:\n",
    "          try:\n",
    "              ch = char_map[c]\n",
    "          except:\n",
    "              append=False\n",
    "      if append:\n",
    "          int_sequence.append(ch)\n",
    "    return int_sequence\n",
    "\n",
    "def int_to_text_sequence(seq):\n",
    "    \"\"\" Use a index map and convert int to a text sequence\n",
    "        >>> from utils import int_to_text_sequence\n",
    "        >>> a = [2,22,10,11,21,2,13,11,6,1,21,2,8,20,17]\n",
    "        >>> b = int_to_text_sequence(a)\n",
    "    \"\"\"\n",
    "    text_sequence = []\n",
    "    for c in seq:\n",
    "        if c == 42: #ctc/pad char\n",
    "            ch = ''\n",
    "        else:\n",
    "            ch = index_map[c]\n",
    "        text_sequence.append(ch)\n",
    "    return text_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_trimmed_model(model, name):\n",
    "\n",
    "    jsonfilename = str(name) + \".json\"\n",
    "    weightsfilename = str(name) + \".h5\"\n",
    "\n",
    "    # # serialize model to JSON\n",
    "    with open(jsonfilename, \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "\n",
    "    # # serialize weights to HDF5\n",
    "    model.save_weights(weightsfilename)\n",
    "\n",
    "    return\n",
    "\n",
    "def save_model(model, name):\n",
    "\n",
    "    if name:\n",
    "        jsonfilename = str(name) + \"/model.json\"\n",
    "        weightsfilename = str(name) + \"/model.h5\"\n",
    "\n",
    "        # # serialize model to JSON\n",
    "        with open(jsonfilename, \"w\") as json_file:\n",
    "            json_file.write(model.to_json())\n",
    "\n",
    "        print(\"Saving model at:\", jsonfilename, weightsfilename)\n",
    "        model.save_weights(weightsfilename)\n",
    "\n",
    "        #save model as combined in single file - contrains arch/weights/config/state\n",
    "        model.save(str(name)+\"/cmodel.h5\")\n",
    "\n",
    "    return\n",
    "\n",
    "def load_model_checkpoint(path, summary=True):\n",
    "\n",
    "    #this is a terrible hack\n",
    "    from keras.utils.generic_utils import get_custom_objects\n",
    "    # get_custom_objects().update({\"tf\": tf})\n",
    "    get_custom_objects().update({\"clipped_relu\": clipped_relu})\n",
    "    get_custom_objects().update({\"selu\": selu})\n",
    "    # get_custom_objects().update({\"TF_NewStatus\": None})\n",
    "\n",
    "    jsonfilename = path+\".json\"\n",
    "    weightsfilename = path+\".h5\"\n",
    "\n",
    "    json_file = open(jsonfilename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "\n",
    "    K.set_learning_phase(1)\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into loaded model\n",
    "    loaded_model.load_weights(weightsfilename)\n",
    "    # loaded_model = load_model(path, custom_objects=custom_objects)\n",
    "\n",
    "\n",
    "    if(summary):\n",
    "        loaded_model.summary()\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "def load_cmodel_checkpoint(path, summary=True):\n",
    "\n",
    "    #this is a terrible hack\n",
    "    from keras.utils.generic_utils import get_custom_objects\n",
    "    # get_custom_objects().update({\"tf\": tf})\n",
    "    get_custom_objects().update({\"clipped_relu\": clipped_relu})\n",
    "    get_custom_objects().update({\"selu\": selu})\n",
    "    # get_custom_objects().update({\"TF_NewStatus\": None})\n",
    "\n",
    "    cfilename = path+\".h5\"\n",
    "\n",
    "    K.set_learning_phase(1)\n",
    "    loaded_model = load_model(cfilename)\n",
    "\n",
    "\n",
    "    if(summary):\n",
    "        loaded_model.summary()\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "memlist = []\n",
    "class MemoryCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        x = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        web_browser_debug = True\n",
    "        #print(x)\n",
    "\n",
    "        if x > 40000:\n",
    "            if web_browser_debug:\n",
    "                if epoch==0:\n",
    "                    start_in_background()\n",
    "                    tr = tracker.SummaryTracker()\n",
    "                    tr.print_diff()\n",
    "            else:\n",
    "                global memlist\n",
    "                all_objects = muppy.get_objects(include_frames=True)\n",
    "                # print(len(all_objects))\n",
    "                sum1 = summary.summarize(all_objects)\n",
    "                memlist.append(sum1)\n",
    "                summary.print_(sum1)\n",
    "                if len(memlist) > 1:\n",
    "                    # compare with last - prints the difference per epoch\n",
    "                    diff = summary.get_diff(memlist[-2], memlist[-1])\n",
    "                    summary.print_(diff)\n",
    "                my_types = muppy.filter(all_objects, Type=types.ClassType)\n",
    "\n",
    "                for t in my_types:\n",
    "                    print(t)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiGcIua7dhXd"
   },
   "source": [
    "**Some import and ctc loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVdZNLRSWDiY"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import random_normal\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "'''\n",
    "This file builds the models\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dense, Activation, Bidirectional, Reshape,Flatten, Lambda, Input,\\\n",
    "    Masking, Convolution1D, BatchNormalization, GRU, Conv1D, RepeatVector, Conv2D,LSTM\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import ZeroPadding1D, Convolution1D, ZeroPadding2D, Convolution2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import TimeDistributed, Dropout\n",
    "from keras.layers.merge import add  # , # concatenate BAD FOR COREML\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.activations import relu\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def selu(x):\n",
    "    # from Keras 2.0.6 - does not exist in 2.0.4\n",
    "   \n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * K.elu(x, alpha)\n",
    "\n",
    "def clipped_relu(x):\n",
    "    return relu(x, max_value=20)\n",
    "\n",
    "# Define CTC loss\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "\n",
    "    # hack for load_model\n",
    "    import tensorflow as tf\n",
    "\n",
    "    ''' from TF: Input requirements\n",
    "    1. sequence_length(b) <= time for all b\n",
    "    2. max(labels.indices(labels.indices[:, 1] == b, 2)) <= sequence_length(b) for all b.\n",
    "    '''\n",
    "\n",
    "    # print(\"CTC lambda inputs / shape\")\n",
    "    # print(\"y_pred:\",y_pred.shape)  # (?, 778, 30)\n",
    "    # print(\"labels:\",labels.shape)  # (?, 80)\n",
    "    # print(\"input_length:\",input_length.shape)  # (?, 1)\n",
    "    # print(\"label_length:\",label_length.shape)  # (?, 1)\n",
    "\n",
    "\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def ctc(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8W3kMrVdZWt"
   },
   "source": [
    "**Batch Generator and feature collection of batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qcnf_LeD8zat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ban1vQiQ8zwk"
   },
   "source": [
    "# Spectrogram Information at bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nBIusaqWQjC"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import python_speech_features as p\n",
    "import scipy.io.wavfile as wav\n",
    "import soundfile\n",
    "import scipy\n",
    "# from aubio import source, pvoc, mfcc\n",
    "from numpy import vstack, zeros, diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from utils import text_to_int_sequence\n",
    "\n",
    "# Data batch generator, responsible for providing the data to fit_generator\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, dataframe, dataproperties, training, batch_size=16, model_input_type=\"mfcc\"):\n",
    "        self.training_data = training\n",
    "        self.model_input_type = model_input_type ##mfcc, mfcc-aubio, spectrogram, spectrogram-img\n",
    "        # self.aubio = False\n",
    "        self.df = dataframe.copy()\n",
    "        #['wav_filesize','transcript','wav_filename']\n",
    "        self.wavpath = self.df['key'].tolist()\n",
    "        self.transcript = self.df['text'].tolist()\n",
    "        self.finish = self.df['duration'].tolist()\n",
    "        self.start = np.zeros(len(self.finish))\n",
    "        self.length = self.finish\n",
    "        self.shuffling = True\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.cur_index = 0\n",
    "\n",
    "        self.feats_std = 0\n",
    "        self.feats_mean = 0\n",
    "\n",
    "        self.set_of_all_int_outputs_used = None\n",
    "\n",
    "        #Free up memory of unneeded data\n",
    "        del dataframe\n",
    "        del dataproperties\n",
    "        self.df = None\n",
    "        del self.df\n",
    "\n",
    "    def normalise(self, feature, eps=1e-14):\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "\n",
    "        batch_x = self.wavpath[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y_trans = self.transcript[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        try:\n",
    "            assert (len(batch_x) == self.batch_size)\n",
    "            assert (len(batch_y_trans) == self.batch_size)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print(batch_x)\n",
    "            #print(batch_y_trans)\n",
    "\n",
    "        # 1. X_data (the MFCC's for the batch)\n",
    "        if(self.model_input_type == \"spectrogram\"):\n",
    "            # 0. get the maximum time length of the batch\n",
    "            x_val = [get_max_specto_time(file_name) for file_name in batch_x]\n",
    "            max_val = max(x_val)\n",
    "            # print(\"Max batch time value is:\", max_val)\n",
    "            X_data = np.array([make_specto_shape(file_name, padlen=max_val) for file_name in batch_x])\n",
    "            assert (X_data.shape == (self.batch_size, max_val, 161))\n",
    "\n",
    "        elif(self.model_input_type == \"mfcc-aubio\"):\n",
    "            x_val = [get_max_aubio(file_name) for file_name in batch_x]\n",
    "            max_val = max(x_val)\n",
    "            # print(\"Max batch time value is:\", max_val)\n",
    "            X_data = np.array([make_aubio_shape(file_name, padlen=max_val) for file_name in batch_x])\n",
    "            # print(X_data.shape)\n",
    "            assert (X_data.shape == (self.batch_size, max_val, 26))\n",
    "\n",
    "        elif(self.model_input_type == \"mfcc\"):\n",
    "            # 0. get the maximum time length of the batch\n",
    "            x_val = [get_max_time(file_name) for file_name in batch_x]\n",
    "            max_val = max(x_val)\n",
    "            # print(\"Max batch time value is:\", max_val)\n",
    "\n",
    "            X_data = np.array([make_mfcc_shape(file_name, padlen=max_val) for file_name in batch_x])\n",
    "            assert (X_data.shape == (self.batch_size, max_val, 26))\n",
    "        # print(\"1. X_data shape:\", X_data.shape)\n",
    "        # print(\"1. X_data:\", X_data)\n",
    "\n",
    "\n",
    "        # 2. labels (made numerical)\n",
    "        # get max label length\n",
    "        y_val = [get_maxseq_len(l) for l in batch_y_trans]\n",
    "        # print(y_val)\n",
    "        max_y = max(y_val)\n",
    "        # print(max_y)\n",
    "        labels = np.array([get_intseq(l, max_intseq_length=max_y) for l in batch_y_trans])\n",
    "        # print(\"2. labels shape:\", labels.shape)\n",
    "        # print(\"2. labels values=\", labels)\n",
    "        assert(labels.shape == (self.batch_size, max_y))\n",
    "\n",
    "        # 3. input_length (required for CTC loss)\n",
    "        # this is the time dimension of CTC (batch x time x mfcc)\n",
    "        #input_length = np.array([get_xsize(mfcc) for mfcc in X_data])\n",
    "        input_length = np.array(x_val)\n",
    "        # print(\"3. input_length shape:\", input_length.shape)\n",
    "        # print(\"3. input_length =\", input_length)\n",
    "        assert(input_length.shape == (self.batch_size,))\n",
    "\n",
    "        # 4. label_length (required for CTC loss)\n",
    "        # this is the length of the number of label of a sequence\n",
    "        #label_length = np.array([len(l) for l in labels])\n",
    "        label_length = np.array(y_val)\n",
    "        # print(\"4. label_length shape:\", label_length.shape)\n",
    "        # print(\"4. label_length =\", label_length)\n",
    "        assert(label_length.shape == (self.batch_size,))\n",
    "\n",
    "        # 5. source_str (used for human readable output on callback)\n",
    "        source_str = np.array([l for l in batch_y_trans])\n",
    "\n",
    "\n",
    "\n",
    "        inputs = {\n",
    "            'the_input': X_data,\n",
    "            'the_labels': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "            'source_str': source_str\n",
    "        }\n",
    "\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def next_batch(self):\n",
    "        while 1:\n",
    "            assert (self.batch_size <= len(self.wavpath))\n",
    "\n",
    "            if (self.cur_index + 1) * self.batch_size >= len(self.wavpath) - self.batch_size:\n",
    "\n",
    "                self.cur_index = 0\n",
    "\n",
    "                if(self.shuffling==True):\n",
    "                    print(\"SHUFFLING as reached end of data\")\n",
    "                    self.genshuffle()\n",
    "\n",
    "            try:\n",
    "                ret = self.get_batch(self.cur_index)\n",
    "            except:\n",
    "                print(\"data error - this shouldn't happen - try next batch\")\n",
    "                self.cur_index += 1\n",
    "                ret = self.get_batch(self.cur_index)\n",
    "\n",
    "            self.cur_index += 1\n",
    "\n",
    "            yield ret\n",
    "\n",
    "    def genshuffle(self):\n",
    "        self.wavpath, self.transcript, self.finish = shuffle(self.wavpath,\n",
    "                                                             self.transcript,\n",
    "                                                             self.finish)\n",
    "\n",
    "\n",
    "    def export_test_mfcc(self):\n",
    "        # this is used to export data e.g. into iOS\n",
    "\n",
    "        testset = next(self.next_batch())[0]\n",
    "        mfcc = testset['the_input'][0:self.batch_size]  ## export all mfcc's in batch #26 x 29 ?\n",
    "        words = testset['source_str'][0:self.batch_size]\n",
    "        labels = testset['the_labels'][0:self.batch_size]\n",
    "\n",
    "        print(\"exporting:\", type(mfcc))\n",
    "        print(mfcc.shape)\n",
    "        print(words.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "        # we save each mfcc/words/label as it's own csv file\n",
    "        for i in range(0, mfcc.shape[0]):\n",
    "            np.savetxt('./Archive/test_spectro/test_spectro_{}.csv'.format(i), mfcc[i, :, :], delimiter=',')\n",
    "\n",
    "        print(words)\n",
    "        print(labels)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "def get_normalise(self, k_samples=100):\n",
    "    # todo use normalise from DS2 - https://github.com/baidu-research/ba-dls-deepspeech\n",
    "    \"\"\" Estimate the mean and std of the features from the training set\n",
    "    Params:\n",
    "        k_samples (int): Use this number of samples for estimation\n",
    "    \"\"\"\n",
    "    # k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "    # samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "    # feats = [self.featurize(s) for s in samples]\n",
    "    # feats = np.vstack(feats)\n",
    "    # self.feats_mean = np.mean(feats, axis=0)\n",
    "    # self.feats_std = np.std(feats, axis=0)\n",
    "    pass\n",
    "\n",
    "def get_maxseq_len(trans):\n",
    "    # PAD\n",
    "    t = text_to_int_sequence(trans)\n",
    "    return len(t)\n",
    "\n",
    "def get_intseq(trans, max_intseq_length=80):\n",
    "    # PAD\n",
    "    t = text_to_int_sequence(trans)\n",
    "    while (len(t) < max_intseq_length):\n",
    "        t.append(27)  # replace with a space char to pad\n",
    "    # print(t)\n",
    "    return t\n",
    "\n",
    "def get_max_time(filename):\n",
    "    fs, audio = wav.read(filename)\n",
    "    r = p.mfcc(audio, samplerate=fs, numcep=26)  # 2D array -> timesamples x mfcc_features\n",
    "    # print(r.shape)\n",
    "    return r.shape[0]  #\n",
    "\n",
    "def get_max_specto_time(filename):\n",
    "    r = spectrogram_from_file(filename)\n",
    "    # print(r.shape)\n",
    "    return r.shape[0]  #\n",
    "\n",
    "\n",
    "def get_max_aubio(filename):\n",
    "    r = aubio(filename)  # 2D array -> timesamples x mfcc_features\n",
    "    # print(r.shape)\n",
    "\n",
    "    return r.shape[0]  #\n",
    "\n",
    "def make_specto_shape(filename, padlen=778):\n",
    "    r = spectrogram_from_file(filename)\n",
    "    t = np.transpose(r)  # 2D array ->  spec x timesamples\n",
    "    X = pad_sequences(t, maxlen=padlen, dtype='float', padding='post', truncating='post').T\n",
    "\n",
    "    return X  # MAXtimesamples x specto {max x 161}\n",
    "\n",
    "def make_aubio_shape(filename, padlen=778):\n",
    "    r = aubio(filename)\n",
    "    t = np.transpose(r)  # 2D array ->  mfcc_features x timesamples\n",
    "    X = pad_sequences(t, maxlen=padlen, dtype='float', padding='post', truncating='post').T\n",
    "    return X  # 2D array -> MAXtimesamples x mfcc_features {778 x 26}\n",
    "\n",
    "def make_mfcc_shape(filename, padlen=778):\n",
    "    fs, audio = wav.read(filename)\n",
    "    r = p.mfcc(audio, samplerate=fs, numcep=26)  # 2D array -> timesamples x mfcc_features\n",
    "    t = np.transpose(r)  # 2D array ->  mfcc_features x timesamples\n",
    "    X = pad_sequences(t, maxlen=padlen, dtype='float', padding='post', truncating='post').T\n",
    "    return X  # 2D array -> MAXtimesamples x mfcc_features {778 x 26}\n",
    "\n",
    "def get_xsize(val):\n",
    "    return val.shape[0]\n",
    "\n",
    "def shuffle_data(self):\n",
    "    self.wavpath, self.transcript, self.finish = shuffle(self.wavpath,\n",
    "                                                         self.transcript,\n",
    "                                                         self.finish)\n",
    "    return\n",
    "\n",
    "\n",
    "def aubio(source_filename):\n",
    "\n",
    "    # print(\"Usage: %s <source_filename> [samplerate] [win_s] [hop_s] [mode]\" % sys.argv[0])\n",
    "    # print(\"  where [mode] can be 'delta' or 'ddelta' for first and second derivatives\")7\n",
    "\n",
    "    n_filters = 40  # must be 40 for mfcc\n",
    "    n_coeffs = 26\n",
    "    win_s = 512\n",
    "    hop_s = win_s // 4\n",
    "    # mode = \"default\"\n",
    "    samplerate = 0\n",
    "\n",
    "    s = source(source_filename, samplerate, hop_s)\n",
    "    samplerate = s.samplerate\n",
    "    p = pvoc(win_s, hop_s)\n",
    "    m = mfcc(win_s, n_filters, n_coeffs, samplerate)\n",
    "\n",
    "    mfccs = zeros([n_coeffs, ])\n",
    "    frames_read = 0\n",
    "    while True:\n",
    "        samples, read = s()\n",
    "        spec = p(samples)\n",
    "        mfcc_out = m(spec)\n",
    "        mfccs = vstack((mfccs, mfcc_out))\n",
    "        frames_read += read\n",
    "        if read < hop_s: break\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "##Require for DS2 - source: https://github.com/baidu-research/ba-dls-deepspeech\n",
    "##############################################################################\n",
    "\n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n",
    "\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                             \" sample rate\")\n",
    "        if step > window:\n",
    "            raise ValueError(\"step size must not be greater than window size\")\n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        pxx, freqs = spectrogram(\n",
    "            audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "            hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "def featurise(audio_clip):\n",
    "    \"\"\" For a given audio clip, calculate the log of its Fourier Transform\n",
    "    Params:\n",
    "        audio_clip(str): Path to the audio clip\n",
    "    \"\"\"\n",
    "\n",
    "    step = 10\n",
    "    window = 20\n",
    "    max_freq = 8000\n",
    "\n",
    "    return spectrogram_from_file(\n",
    "        audio_clip, step=step, window=window,\n",
    "        max_freq=max_freq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTXn-erFdCji"
   },
   "source": [
    "**Functions to calculate WER, LER and to correct predicted transcript from KenLM language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cAcfoHWWSbs"
   },
   "outputs": [],
   "source": [
    "# this file is an adaptation from the work at mozilla deepspeech github.com/mozilla/DeepSpeech\n",
    "\n",
    "\n",
    "import kenlm\n",
    "import re\n",
    "from heapq import heapify\n",
    "\n",
    "def wer(original, result):\n",
    "    r\"\"\"\n",
    "    The WER is defined as the editing/Levenshtein distance on word level\n",
    "    divided by the amount of words in the original text.\n",
    "    In case of the original having more words (N) than the result and both\n",
    "    being totally different (all N words resulting in 1 edit operation each),\n",
    "    the WER will always be 1 (N / N = 1).\n",
    "    \"\"\"\n",
    "    # The WER ist calculated on word (and NOT on character) level.\n",
    "    # Therefore we split the strings into words first:\n",
    "    original = original.split()\n",
    "    result = result.split()\n",
    "    return levenshtein(original, result) / float(len(original))\n",
    "\n",
    "def wers(originals, results):\n",
    "    count = len(originals)\n",
    "    try:\n",
    "        assert count > 0\n",
    "    except:\n",
    "        print(originals)\n",
    "        raise(\"ERROR assert count>0 - looks like data is missing\")\n",
    "    rates = []\n",
    "    mean = 0.0\n",
    "    assert count == len(results)\n",
    "    for i in range(count):\n",
    "        rate = wer(originals[i], results[i])\n",
    "        mean = mean + rate\n",
    "        rates.append(rate)\n",
    "    return rates, mean / float(count)\n",
    "\n",
    "def lers(originals, results):\n",
    "    count = len(originals)\n",
    "    assert count > 0\n",
    "    rates = []\n",
    "    norm_rates = []\n",
    "\n",
    "    mean = 0.0\n",
    "    norm_mean = 0.0\n",
    "\n",
    "    assert count == len(results)\n",
    "    for i in range(count):\n",
    "        rate = levenshtein(originals[i], results[i])\n",
    "        mean = mean + rate\n",
    "\n",
    "        normrate = (float(rate) / len(originals[i]))\n",
    "\n",
    "        norm_mean = norm_mean + normrate\n",
    "\n",
    "        rates.append(rate)\n",
    "        norm_rates.append(round(normrate, 4))\n",
    "\n",
    "    return rates, (mean / float(count)), norm_rates, (norm_mean/float(count))\n",
    "\n",
    "\n",
    "# The following code is from: http://hetland.org/coding/python/levenshtein.py\n",
    "def levenshtein(a,b):\n",
    "    \"Calculates the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "\n",
    "    current = list(range(n+1))\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "\n",
    "    return current[n]\n",
    "\n",
    "\n",
    "\n",
    "# Lazy-load language model (TED corpus, Kneser-Ney, 4-gram, 30k word LM)\n",
    "def get_model():\n",
    "    global MODEL\n",
    "    if MODEL is None:\n",
    "        #MODEL = kenlm.Model('./lm/timit-lm.klm')\n",
    "        MODEL = kenlm.Model('/content/drive/MyDrive/WordBoundaries/WordBoundaries.arpa')\n",
    "    return MODEL\n",
    "\n",
    "\n",
    "def words(text):\n",
    "    \"List of words in text.\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "def log_probability(sentence):\n",
    "    \"Log base 10 probability of `sentence`, a list of words\"\n",
    "    return get_model().score(' '.join(sentence), bos=False, eos=False)\n",
    "\n",
    "\n",
    "def correction(sentence):\n",
    "    \"Most probable spelling correction for sentence.\"\n",
    "    BEAM_WIDTH = 1024\n",
    "    layer = [(0, [])]\n",
    "    for word in words(sentence):\n",
    "        layer = [(-log_probability(node + [cword]), node + [cword]) for cword in candidate_words(word) for\n",
    "                 priority, node in layer]\n",
    "        heapify(layer)\n",
    "        layer = layer[:BEAM_WIDTH]\n",
    "    return ' '.join(layer[0][1])\n",
    "\n",
    "\n",
    "def candidate_words(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known_words([word]) or known_words(edits1(word)) or known_words(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known_words(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters = \"\".join(list(char_map.keys())[1:])\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# globals\n",
    "\n",
    "MODEL = None\n",
    "# Load known word set\n",
    "with open('/content/drive/MyDrive/WordBoundaries/Pound Transcription-CISAMPA.txt') as f:\n",
    "    WORDS = set(words(f.read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al5IDGsNc4HG"
   },
   "source": [
    "Call after every epoch to calculate WER from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNMdsRJ5WVMm"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "# from text import *\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import keras.backend as K\n",
    "\n",
    "# from utils import save_model, int_to_text_sequence\n",
    "\n",
    "class ReportCallback(callbacks.Callback):\n",
    "    def __init__(self, test_func, validdata, model, runtimestr, save):\n",
    "        self.test_func = test_func\n",
    "\n",
    "        self.validdata = validdata\n",
    "        self.validdata_next_val = self.validdata.next_batch()\n",
    "        self.batch_size = validdata.batch_size\n",
    "        self.save = save\n",
    "\n",
    "        # useful if you want to decrease amount in validation\n",
    "        self.valid_test_devide = 1  # 1=no reduce, 10 = 1/10th\n",
    "        #if socket.gethostname().lower() in 'rs-e5550'.lower(): self.valid_test_devide = 50\n",
    "\n",
    "        self.val_best_mean_ed = 0\n",
    "        self.val_best_norm_mean_ed = 0\n",
    "\n",
    "        self.lm = get_model()\n",
    "\n",
    "        self.model = model\n",
    "        self.runtimestr = runtimestr\n",
    "\n",
    "        self.mean_wer_log = []\n",
    "        self.mean_ler_log = []\n",
    "        self.norm_mean_ler_log = []\n",
    "\n",
    "        self.earlystopping = True\n",
    "        self.shuffle_epoch_end = True\n",
    "        self.force_output = False\n",
    "\n",
    "    def validate_epoch_end(self, verbose=0):\n",
    "\n",
    "        originals = []\n",
    "        results = []\n",
    "        count = 0\n",
    "        self.validdata.cur_index = 0  # reset index\n",
    "\n",
    "        if self.valid_test_devide: #check not zero\n",
    "            allvalid = (len(self.validdata.wavpath) // self.validdata.batch_size) // self.valid_test_devide\n",
    "\n",
    "\n",
    "        #make a pass through all the validation data and assess score\n",
    "        for c in range(0, allvalid):\n",
    "\n",
    "            word_batch = next(self.validdata_next_val)[0]\n",
    "            decoded_res = decode_batch(self.test_func,\n",
    "                                       word_batch['the_input'][0:self.batch_size],\n",
    "                                       self.batch_size)\n",
    "\n",
    "            for j in range(0, self.batch_size):\n",
    "                # print(c,j)\n",
    "                count += 1\n",
    "                decode_sent = decoded_res[j]\n",
    "                corrected = correction(decode_sent)\n",
    "                label = word_batch['source_str'][j]\n",
    "                #print(label)\n",
    "\n",
    "                if verbose:\n",
    "                    cor_wer = wer(label, corrected)\n",
    "                    dec_wer = wer(label, decode_sent)\n",
    "\n",
    "                    if(dec_wer < 0.4 or cor_wer < 0.4 or self.force_output):\n",
    "                        print(\"\\n{}.GroundTruth:{}\\n{}.Transcribed:{}\\n{}.LMCorrected:{}\".format(str(j), label,\n",
    "                                                                                     str(j), decode_sent,\n",
    "                                                                                     str(j), corrected))\n",
    "\n",
    "                    # print(\"Sample Decoded WER:{}, Corrected LM WER:{}\".format(dec_wer, cor_wer))\n",
    "\n",
    "                originals.append(label)\n",
    "                results.append(corrected)\n",
    "\n",
    "        print(\"########################################################\")\n",
    "        print(\"Completed Validation Test: WER & LER results\")\n",
    "        rates, mean = wers(originals, results)\n",
    "        # print(\"WER rates     :\", rates)\n",
    "        lrates, lmean, norm_lrates, norm_lmean = lers(originals, results)\n",
    "        # print(\"LER rates     :\", lrates)\n",
    "        # print(\"LER norm rates:\", norm_lrates)\n",
    "        # print(\"########################################################\")\n",
    "        print(\"Test WER average is   :\", mean)\n",
    "        print(\"Test LER average is   :\", lmean)\n",
    "        print(\"Test normalised LER is:\", norm_lmean)\n",
    "        print(\"########################################################\")\n",
    "        # print(\"(note both WER and LER use LanguageModel not raw output)\")\n",
    "\n",
    "        self.mean_wer_log.append(mean)\n",
    "        self.mean_ler_log.append(lmean)\n",
    "        self.norm_mean_ler_log.append(norm_lmean)\n",
    "\n",
    "        #delete all values?\n",
    "        # del originals, results, count, allvalid\n",
    "        # del word_batch, decoded_res\n",
    "        # del decode_sent,\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        K.set_learning_phase(0)\n",
    "\n",
    "        if(self.shuffle_epoch_end):\n",
    "            print(\"shuffle_epoch_end\")\n",
    "            self.validdata.genshuffle()\n",
    "\n",
    "\n",
    "        self.validate_epoch_end(verbose=1)\n",
    "\n",
    "        if self.save:\n",
    "            #check to see lowest wer/ler on prev values\n",
    "            # if(len(self.mean_wer_log)>=2):\n",
    "            #     lastWER = self.mean_wer_log[-1]\n",
    "            #     allWER = np.min(self.mean_wer_log[:-1])\n",
    "            #     lastLER = self.mean_ler_log[-1]\n",
    "            #     allLER = np.min(self.mean_ler_log[:-1])\n",
    "\n",
    "                # if(lastLER < allLER or lastWER < allWER):\n",
    "            savedir = \"/content/drive/My Drive/Saved_Phonetic_Models/epoch/LER-WER-best-{}\".format(self.runtimestr+'-'+str(round(self.mean_wer_log[-1],2)))\n",
    "            print(\"better ler/wer at:\", savedir)\n",
    "            if not os.path.isdir(savedir):\n",
    "                os.makedirs(savedir)\n",
    "            try:\n",
    "                save_model(self.model, name=savedir)\n",
    "            except Exception as e:\n",
    "                print(\"couldn't save error:\", e)\n",
    "\n",
    "            #early stopping if VAL WER worse 4 times in a row\n",
    "            if(len(self.mean_wer_log)>10 and self.earlystopping):\n",
    "                if(earlyStopCheck(self.mean_wer_log[-10:])):\n",
    "                    print(\"EARLY STOPPING\")\n",
    "\n",
    "                    print(\"Mean WER   :\", self.mean_wer_log)\n",
    "                    print(\"Mean LER   :\", self.mean_ler_log)\n",
    "                    print(\"NormMeanLER:\", self.norm_mean_ler_log)\n",
    "\n",
    "                    sys.exit()\n",
    "\n",
    "\n",
    "        #activate learning phase - incase keras doesn't\n",
    "        K.set_learning_phase(1)\n",
    "\n",
    "\n",
    "def decode_batch(test_func, word_batch, batch_size):\n",
    "    ret = []\n",
    "    output = test_func([word_batch])[0] #16xTIMEx29 = batch x time x classes\n",
    "    greedy = True\n",
    "    merge_chars = True\n",
    "\n",
    "    for j in range(batch_size):  # 0:batch_size\n",
    "\n",
    "        if greedy:\n",
    "            out = output[j]\n",
    "            #print(out)\n",
    "            best = list(np.argmax(out, axis=1))\n",
    "\n",
    "            if merge_chars:\n",
    "                merge = [k for k,g in itertools.groupby(best)]\n",
    "\n",
    "            else:\n",
    "                raise (\"not implemented no merge\")\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "            raise(\"not implemented beam\")\n",
    "\n",
    "        try:\n",
    "            #print(merge)\n",
    "            outStr = int_to_text_sequence(merge)\n",
    "            #print(\"outStr \",outStr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Unrecognised character on decode error:\", e)\n",
    "            outStr = \"DECODE ERROR:\"+str(best)\n",
    "            raise(\"DECODE ERROR2\")\n",
    "\n",
    "        ret.append(''.join(outStr))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def earlyStopCheck(array):\n",
    "    last = array[-1]\n",
    "    rest = array[:-1]\n",
    "    print(last, \" vs \", rest)\n",
    "\n",
    "    #in other words- the last element is bigger than all 4 of the previous, therefore early stopping required\n",
    "    if all(i <= last for i in rest):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qUUS2DmWgRF"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import socket\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "# from data import combine_all_wavs_and_trans_from_csvs\n",
    "# from generator import BatchGenerator\n",
    "# from model import *\n",
    "# from report import ReportCallback\n",
    "# from utils import load_model_checkpoint, save_model, MemoryCallback\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "# Prevent pool_allocator message\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbAYxTjJWiCa",
    "outputId": "251b2840-1d6a-415b-995b-5a6600a6e24a"
   },
   "outputs": [],
   "source": [
    "train_files=\"/content/drive/MyDrive/WordBoundaries/WordBound-rumi_csalt.csv\"\n",
    "valid_files=\"/content/drive/MyDrive/WordBoundaries/WordBound-validate_rumi_csalt.csv\"\n",
    "train_dataprops, df_train = combine_all_wavs_and_trans_from_csvs(train_files, sortagrad=True,start=-100,delBigTranscripts=True)\n",
    "valid_dataprops, df_valid = combine_all_wavs_and_trans_from_csvs(valid_files, sortagrad=True,start=-100,delBigTranscripts=True)\n",
    "\n",
    "model_arch=2\n",
    "# check any special data model requirments e.g. a spectrogram\n",
    "if(model_arch == 1):\n",
    "    model_input_type = \"mfcc\"\n",
    "elif(model_arch == 2 or args.model_arch == 5):\n",
    "    print(\"Spectrogram required\")\n",
    "    # spectrogram = True\n",
    "    model_input_type = \"spectrogram\"\n",
    "else:\n",
    "    model_input_type = \"mfcc\"\n",
    "\n",
    "\n",
    "## 2. init data generators\n",
    "print(\"Creating data batch generators\")\n",
    "batchsize=16\n",
    "traindata = BatchGenerator(dataframe=df_train, dataproperties=train_dataprops,\n",
    "                          training=True, batch_size=batchsize, model_input_type=model_input_type)\n",
    "validdata = BatchGenerator(dataframe=df_valid, dataproperties=valid_dataprops,\n",
    "                          training=False, batch_size=batchsize, model_input_type=model_input_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaHsY-UJcrBj"
   },
   "source": [
    " **Our Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxbbENW3Wm-I"
   },
   "outputs": [],
   "source": [
    "# def cnn_output_length(input_length, filter_size, border_mode, stride, dilation=1):\n",
    "#     \"\"\" Compute the length of the output sequence after 1D convolution along\n",
    "#         time. Note that this function is in line with the function used in\n",
    "#         Convolution1D class from Keras.\n",
    "#     Params:\n",
    "#         input_length (int): Length of the input sequence.\n",
    "#         filter_size (int): Width of the convolution kernel.\n",
    "#         border_mode (str): `same`, `valid`, `causal`.\n",
    "#         stride (int): Stride size used in 1D convolution.\n",
    "#         dilation (int)\n",
    "#     \"\"\"\n",
    "#     if input_length is None:\n",
    "#         return None\n",
    "#     assert border_mode in {'same', 'valid', 'causal'}\n",
    "#     if border_mode == 'same' or border_mode == 'valid': dilation = 1\n",
    "#     dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "#     if border_mode == 'same':\n",
    "#         output_length = input_length\n",
    "#     elif border_mode == 'valid' or border_mode == 'causal':\n",
    "#         output_length = input_length - dilated_filter_size + 1\n",
    "#     return (output_length + stride - 1) // stride\n",
    "def ds2_gru_model(input_dim=161, fc_size=1024, rnn_size=512, output_dim=29, initialization='glorot_uniform',\n",
    "                  conv_layers=1, gru_layers=2, use_conv=False):\n",
    "    \"\"\" DeepSpeech 2 implementation\n",
    "\n",
    "    Architecture:\n",
    "        Input Spectrogram TIMEx161\n",
    "        1 Batch Normalisation layer on input\n",
    "        1-3 Convolutional Layers\n",
    "        1 Batch Normalisation layer\n",
    "        1-7 BiDirectional GRU Layers\n",
    "        1 Batch Normalisation layer\n",
    "        1 Fully connected Dense\n",
    "        1 Softmax output\n",
    "\n",
    "    Details:\n",
    "       - Uses Spectrogram as input rather than MFCC\n",
    "       - Did not use BN on the first input\n",
    "       - Network does not dynamically adapt to maximum audio size in the first convolutional layer. Max conv\n",
    "          length padded at 2048 chars, otherwise use_conv=False\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1512.02595\n",
    "    \"\"\"\n",
    "\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    input_data = Input(shape=(None, input_dim), name='the_input')\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(input_data)\n",
    "\n",
    "    if use_conv:\n",
    "        x = ZeroPadding1D(padding=(0, 2048))(x)\n",
    "        for l in range(conv_layers):\n",
    "            x = Conv1D(filters=fc_size[l], name='conv_{}'.format(l+1), kernel_size=11, padding='valid', activation='relu', strides=2)(x)\n",
    "    else:\n",
    "        for l in range(conv_layers):\n",
    "            x = TimeDistributed(Dense(fc_size[l], name='fc_{}'.format(l + 1), activation='relu'))(x)  # >>(?, time, fc_size)\n",
    "    # conv=x\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)\n",
    "\n",
    "    for l in range(gru_layers):\n",
    "        x = Bidirectional(GRU(rnn_size[l], name='fc_{}'.format(l + 1), return_sequences=True, activation='relu', kernel_initializer=initialization),\n",
    "                      merge_mode='sum')(x)\n",
    "\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)\n",
    "    # x,_=AttentionLayer()([conv,x])\n",
    "    # Last Layer 5+6 Time Dist Dense Layer & Softmax\n",
    "    x = TimeDistributed(Dense(fc_size[-1], activation=clipped_relu))(x)\n",
    "    y_pred = TimeDistributed(Dense(output_dim, name=\"y_pred\", activation=\"softmax\"))(x)\n",
    "    model_ = Model(inputs=input_data, outputs=y_pred)\n",
    "    # model_.output_length = lambda x: cnn_output_length(x, 11, 'valid', 2)\n",
    "    # print(model_.output_length)\n",
    "    # labels = K.placeholder(name='the_labels', ndim=1, dtype='int32')\n",
    "    labels = Input(name='the_labels', shape=[None,], dtype='int32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int32')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int32')\n",
    "    # output_lengths = Lambda(model_.output_length)(input_length)\n",
    "    # print(output_lengths.shape)\n",
    "    # print(input_length.shape)\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred,\n",
    "                                                                       labels,\n",
    "                                                                       input_length,\n",
    "                                                                       label_length])\n",
    "\n",
    "    model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOCjlXCoEi78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyFOj0t8EjWN"
   },
   "source": [
    "If first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "pPRUR3ZKWuja",
    "outputId": "dd830f7a-681f-4b60-d09c-1d4fa34803b3"
   },
   "outputs": [],
   "source": [
    "runtime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "name=\"DS\"+str(2)+\"_\"+runtime\n",
    "output_dir = os.path.join('/content/drive/My Drive/Saved_Phonetic_Models',\n",
    "                              'model%s_%s' % (model_arch,\n",
    "                                          name))\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "## 3. Load existing or create new model\n",
    "loadcheckpointpath='/content/drive/MyDrive/WordBoundaries/WordBoundaryModels/epoch/LER-WER-best-DS2_2020-12-30_17-17-100.3'\n",
    "if False:\n",
    "  print('lodaing checkpoint')\n",
    "  # load existing\n",
    "  print(\"Loading model\")\n",
    "  # cp = loadcheckpointpath\n",
    "  assert(os.path.isdir(loadcheckpointpath))\n",
    "\n",
    "  model_path = os.path.join(loadcheckpointpath, \"model\")\n",
    "    # assert(os.path.isfile(model_path))\n",
    "\n",
    "  model = load_model_checkpoint(model_path)\n",
    "else:\n",
    "  print(\"Inside Else\")\n",
    "  model = ds2_gru_model(input_dim=161, fc_size=[256] ,rnn_size=[256,512], output_dim=67)\n",
    "  print('Model Created')\n",
    "# model = ds1(input_dim=26, fc_size=512, rnn_size=512, output_dim=43)\n",
    "\n",
    "print(model.summary(line_length=80))\n",
    "\n",
    "#required to save the JSON\n",
    "# save_model(model, output_dir)\n",
    "opt='sgd'\n",
    "if (opt.lower() == 'sgd'):\n",
    "    opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "elif (opt.lower() == 'adam'):\n",
    "    opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "elif (opt.lower() == 'nadam'):\n",
    "    opt = Nadam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=5)\n",
    "else:\n",
    "    raise \"optimiser not recognised\"\n",
    "\n",
    "model.compile(optimizer=opt, loss=ctc,metrics=['mse'])\n",
    "\n",
    "## 4. train\n",
    "train_steps=0\n",
    "if train_steps == 0:\n",
    "    train_steps = len(df_train.index) // batchsize\n",
    "    # print(args.train_steps)\n",
    "# we use 1/xth of the validation data at each epoch end to test val score\n",
    "valid_steps=0\n",
    "if valid_steps == 0:\n",
    "\n",
    "    valid_steps = (len(df_valid.index) // batchsize)\n",
    "    # print(args.valid_steps)\n",
    "\n",
    "memcheck=False\n",
    "if memcheck:\n",
    "    cb_list = [MemoryCallback()]\n",
    "else:\n",
    "    cb_list = []\n",
    "tensorboard=True\n",
    "if tensorboard:\n",
    "    tb_cb = TensorBoard(log_dir='/content/drive/My Drive/tensorboard/{}/'.format(name), write_graph=False, write_images=True)\n",
    "    cb_list.append(tb_cb)\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSMQRCtxWz2X"
   },
   "source": [
    "\n",
    "\n",
    "> **Training will start after running the cell below**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "MCXkpV5fWvIt",
    "outputId": "6511b8a0-5554-425a-a27d-dd7cab5482c0"
   },
   "outputs": [],
   "source": [
    "y_pred = model.get_layer('ctc').input[0]\n",
    "input_data = model.get_layer('the_input').input\n",
    "\n",
    "report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "report_cb = ReportCallback(report, validdata, model, name, save=True)\n",
    "# model.load_weights(\"/content/drive/My Drive/models_urdu/epoch/LER-WER-best-DS2_2020-09-29_12-13-0.11/model.h5\")\n",
    "cb_list.append(report_cb)\n",
    "history=model.fit_generator(generator=traindata.next_batch(),\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    epochs=1,\n",
    "                    callbacks=cb_list,\n",
    "                    validation_data=validdata.next_batch(),\n",
    "                    validation_steps=valid_steps,\n",
    "                    initial_epoch=0,\n",
    "                    verbose=1,\n",
    "                    class_weight=None,\n",
    "                    max_q_size=10,\n",
    "                    workers=1,\n",
    "                    pickle_safe=False\n",
    "                    )\n",
    "\n",
    "# K.clear_seggsion()\n",
    "\n",
    "## These are the most important metrics\n",
    "print(\"Mean WER   :\", report_cb.mean_wer_log)\n",
    "print(\"Mean LER   :\", report_cb.mean_ler_log)\n",
    "print(\"NormMeanLER:\", report_cb.norm_mean_ler_log)\n",
    "\n",
    "# export to csv?\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgffltB6cHA9"
   },
   "source": [
    "\n",
    "\n",
    ">#  **Testing of validation files**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwL1vdFQb3j5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import socket\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def main_test(test_files,name,batchsize,model_arch,loadcheckpointpath,opt,train_steps,valid_steps):\n",
    "# def main(args):\n",
    "    '''\n",
    "\n",
    "    only args.name args.test_files and args.loadcheckpointpath can be passed as args\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    print(\"Getting data from arguments\")\n",
    "    test_dataprops, df_test = combine_all_wavs_and_trans_from_csvs(test_files, sortagrad=False,start=0,end=100)\n",
    "\n",
    "\n",
    "    # check any special data model requirments e.g. a spectrogram\n",
    "    if(model_arch == 1):\n",
    "        model_input_type = \"mfcc\"\n",
    "    elif(model_arch == 2 or model_arch == 5):\n",
    "        print(\"Spectrogram required\")\n",
    "        # spectrogram = True\n",
    "        model_input_type = \"spectrogram\"\n",
    "    else:\n",
    "        model_input_type = \"mfcc\"\n",
    "\n",
    "\n",
    "\n",
    "    ## 2. init data generators\n",
    "    print(\"Creating data batch generators\")\n",
    "    testdata = BatchGenerator(dataframe=df_test, dataproperties=test_dataprops,\n",
    "                               training=False, batch_size=1, model_input_type=model_input_type)\n",
    "\n",
    "\n",
    "\n",
    "    ## 3. Load existing or error\n",
    "    if loadcheckpointpath:\n",
    "        # load existing\n",
    "        print(\"Loading model\")\n",
    "\n",
    "        cp = loadcheckpointpath\n",
    "        assert(os.path.isdir(cp))\n",
    "        trimmed = False\n",
    "\n",
    "        if trimmed:\n",
    "            model_path = os.path.join(cp, \"TRIMMED_ds_model\")\n",
    "        else:\n",
    "            model_path = os.path.join(cp, \"model\")\n",
    "        # assert(os.path.isfile(model_path))\n",
    "\n",
    "        model = load_model_checkpoint(model_path)\n",
    "        opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    else:\n",
    "        # new model\n",
    "        raise(\"You need to load an existing trained model\")\n",
    "\n",
    "\n",
    "    model.compile(optimizer=opt, loss=ctc)\n",
    "\n",
    "    ## 4. test\n",
    "\n",
    "    train_steps = len(df_test.index) // 200\n",
    "\n",
    "    try:\n",
    "        y_pred = model.get_layer('ctc').input[0]\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)\n",
    "        print(\"couldn't find ctc layer, possibly a trimmed layer, trying other name\")\n",
    "        y_pred = model.get_layer('out').output\n",
    "\n",
    "    input_data = model.get_layer('the_input').input\n",
    "\n",
    "    K.set_learning_phase(0)\n",
    "    report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "    report_cb = ReportCallback(report, testdata, model, name, save=False)\n",
    "    report_cb.force_output = True\n",
    "    print('starting....................................')\n",
    "    report_cb.on_epoch_end(0, logs=None)\n",
    "\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm8jcJZ-cWzF"
   },
   "source": [
    "**Change loadcheckpointpath to the last best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mw9S_ZWFb1O4",
    "outputId": "cb6bf9cd-0bfe-4a42-8856-5169d449c3aa"
   },
   "outputs": [],
   "source": [
    "runtime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "name=\"DS\"+str(2)+\"_\"+runtime\n",
    "#     print(args)\n",
    "\n",
    "main_test(test_files=\"/content/drive/MyDrive/Phoneme-validate_rumi_csalt.csv\",model_arch=2,name=name,batchsize=8,train_steps=0,valid_steps=0,loadcheckpointpath='/content/drive/MyDrive/Saved_Phonetic_Models/epoch/LER-WER-best-DS2_2021-01-25_12-58-1.01',opt='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2q6RcsTBmpE",
    "outputId": "f26fae33-ebe4-447c-d4d3-9eeb566ed22b"
   },
   "outputs": [],
   "source": [
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-SlxZR-BobN"
   },
   "outputs": [],
   "source": [
    "!mkdir kenlm/build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y25isb8iBuSf",
    "outputId": "34c9cd37-bf75-4f82-926b-5994366e52a2"
   },
   "outputs": [],
   "source": [
    "%cd kenlm/build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CcpDmR4IByjY",
    "outputId": "f2ce34f7-952d-428a-c3f0-3d2292ddf4c2"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXmFYw99B1YN",
    "outputId": "5f166a52-d0aa-4f51-bbfd-2aef1908f0f1"
   },
   "outputs": [],
   "source": [
    "!cmake .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRjV5PycB_Nx",
    "outputId": "aab2abeb-e345-4397-c7b1-90f538a11d0d"
   },
   "outputs": [],
   "source": [
    "!make -j2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vK-etsKhCEbK",
    "outputId": "4cfe3a66-124e-43ae-e812-a2ca5c3cb2c8"
   },
   "outputs": [],
   "source": [
    "!bin/lmplz -o 5 </content/drive/MyDrive/UrduPhoneticSpeechCorpus/Cleaned-CISAMPA.txt >/content/PH_WBA.arpa --discount_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_uWDMzsuAyQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "#input file\n",
    "fin = open(\"/content/final_lm_cleaned2.txt\", \"rt\")\n",
    "#output file to write the result to\n",
    "fout = open(\"/content/final_lm_cleaned3.txt\", \"wt\")\n",
    "#for each line in the input file\n",
    "for line in fin:\n",
    "  s1=re.sub(\"\\\"\",\"\",line)\n",
    "  fout.write(s1)\n",
    "#close input and output files\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YU-Kx47mvifr",
    "outputId": "3657612c-dcd4-4160-d6e4-d50e7d7ff162"
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1AUXRFtUBVA",
    "outputId": "29f920e3-cb4c-405e-eb9a-2b9e4e8c83ec"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhdidxhyUFbL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Phoneme Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
